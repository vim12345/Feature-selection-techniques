# Feature-selection-techniques

# Description:
Feature selection helps identify the most relevant features for a model by eliminating irrelevant, redundant, or noisy data. This improves model performance, interpretability, and reduces overfitting. In this project, we demonstrate multiple common feature selection techniques on the Iris dataset.

# ğŸ” Techniques Demonstrated:
* Univariate Selection (ANOVA F-test) â€“ Select features individually based on statistical tests.

 * Recursive Feature Elimination (RFE) â€“ Recursively removes less important features.

* Tree-Based Feature Importance â€“ Uses models like Random Forest to rank features.

These techniques can be combined with cross-validation, or used with other datasets like customer churn or credit scoring.
