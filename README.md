# Feature-selection-techniques

# Description:
Feature selection helps identify the most relevant features for a model by eliminating irrelevant, redundant, or noisy data. This improves model performance, interpretability, and reduces overfitting. In this project, we demonstrate multiple common feature selection techniques on the Iris dataset.

# 🔍 Techniques Demonstrated:
* Univariate Selection (ANOVA F-test) – Select features individually based on statistical tests.

 * Recursive Feature Elimination (RFE) – Recursively removes less important features.

* Tree-Based Feature Importance – Uses models like Random Forest to rank features.

These techniques can be combined with cross-validation, or used with other datasets like customer churn or credit scoring.
